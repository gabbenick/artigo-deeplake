{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f3dc7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm  # Use tqdm.notebook for better Jupyter integration\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Make sure you have these installed:\n",
    "# pip install pytorch-metric-learning scikit-learn deeplake\n",
    "from pytorch_metric_learning import losses, miners\n",
    "import deeplake\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4ddca02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Experiment Parameters ---\n",
    "# We are now using the full dataset, so these are no longer needed\n",
    "# NUM_TRAIN_SAMPLES = 1000\n",
    "# NUM_TEST_SAMPLES = 200\n",
    "TEST_SPLIT_RATIO = 0.2 # Use 20% of the data for testing\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = 128\n",
    "# Let's reduce epochs for a faster full run, or keep it if you have time\n",
    "NUM_EPOCHS = 50\n",
    "EMBEDDING_DIM = 64\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_WORKERS = 0 # Keep at 0 for stability on CPU\n",
    "\n",
    "# --- PATHS (These should be correct) ---\n",
    "DEEPLAKE_PATH = './dl_datasets/shdataset_12k'\n",
    "LOCAL_PNG_ROOT = './db/SHdataset_12k' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3387757e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 3 (Full Dataset): Creating canonical file order ---\n",
      "Manually iterating to get all 'train' filenames...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d0a985bbbbf471aa79627848936e480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Manually filtering:   0%|          | 0/12051 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found and sorted 9641 total filenames in the 'train' split.\n",
      "Splitting into 7713 training samples and 1928 testing samples.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Step 3 (Full Dataset): Creating canonical file order ---\")\n",
    "\n",
    "def safe_get_tensor_item_value(tensor_item):\n",
    "    \"\"\"Safely extracts a Python native value from a single item from a Deep Lake tensor.\"\"\"\n",
    "    if hasattr(tensor_item, 'text') and callable(tensor_item.text): return tensor_item.text()\n",
    "    if isinstance(tensor_item, (str, int, float)): return tensor_item\n",
    "    if hasattr(tensor_item, 'numpy') and callable(tensor_item.numpy):\n",
    "        np_val = tensor_item.numpy()\n",
    "        if np_val.size == 1: return np_val.item() if np_val.ndim == 0 else np_val[0]\n",
    "        return np_val \n",
    "    if isinstance(tensor_item, np.ndarray):\n",
    "        if tensor_item.size == 1: return tensor_item.item() if tensor_item.ndim == 0 else tensor_item[0]\n",
    "        return tensor_item\n",
    "    try: return str(tensor_item)\n",
    "    except Exception: return \"ErrorConvertingItem\"\n",
    "\n",
    "# 1. Load the dataset using deeplake.open()\n",
    "ds = deeplake.open(DEEPLAKE_PATH)\n",
    "\n",
    "# 2. Manually iterate to get all filenames from the 'train' split\n",
    "print(\"Manually iterating to get all 'train' filenames...\")\n",
    "filenames_list = []\n",
    "for i in tqdm(range(len(ds)), desc=\"Manually filtering\"):\n",
    "    if safe_get_tensor_item_value(ds[i]['split']) == 'train':\n",
    "        filenames_list.append(safe_get_tensor_item_value(ds[i]['original_filename']))\n",
    "\n",
    "# 3. Sort the extracted list to have a canonical order\n",
    "all_filenames = sorted(filenames_list)\n",
    "print(f\"\\nFound and sorted {len(all_filenames)} total filenames in the 'train' split.\")\n",
    "\n",
    "# 4. Dynamically create train/test splits from the full list\n",
    "num_total_files = len(all_filenames)\n",
    "num_test_files = int(num_total_files * TEST_SPLIT_RATIO)\n",
    "num_train_files = num_total_files - num_test_files\n",
    "\n",
    "train_filenames = all_filenames[:num_train_files]\n",
    "test_filenames = all_filenames[num_train_files:]\n",
    "\n",
    "print(f\"Splitting into {len(train_filenames)} training samples and {len(test_filenames)} testing samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "930365ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 4: Define Shared Components (Corrected) ---\n",
    "\n",
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self, embedding_dim=EMBEDDING_DIM):\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "        self.resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        num_ftrs = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Linear(num_ftrs, embedding_dim)\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# FINAL CORRECTED VERSION of evaluate_model\n",
    "def evaluate_model(model, train_loader_for_eval, test_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluates the model and returns a dictionary with metrics, the trained k-NN,\n",
    "    and the embeddings/labels used for visualization.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Evaluation ---\")\n",
    "    model.eval()\n",
    "    \n",
    "    # Generate embeddings for the training set (to train the k-NN)\n",
    "    train_embeddings, train_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(train_loader_for_eval, desc=\"Generating train embeddings\"):\n",
    "            embeddings = model(images.to(device))\n",
    "            train_embeddings.append(embeddings.cpu().numpy())\n",
    "            train_labels.append(labels.numpy())\n",
    "    train_embeddings = np.concatenate(train_embeddings)\n",
    "    train_labels = np.concatenate(train_labels)\n",
    "\n",
    "    # Generate embeddings for the test set (to evaluate the k-NN)\n",
    "    test_embeddings, test_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc=\"Generating test embeddings\"):\n",
    "            embeddings = model(images.to(device))\n",
    "            test_embeddings.append(embeddings.cpu().numpy())\n",
    "            test_labels.append(labels.numpy())\n",
    "    test_embeddings = np.concatenate(test_embeddings)\n",
    "    test_labels = np.concatenate(test_labels)\n",
    "\n",
    "    # Train k-NN and predict\n",
    "    print(\"Training k-NN classifier on learned embeddings...\")\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn.fit(train_embeddings, train_labels)\n",
    "    predictions = knn.predict(test_embeddings)\n",
    "    pred_probs = knn.predict_proba(test_embeddings)[:, 1]\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "    f1 = f1_score(test_labels, predictions)\n",
    "    auc = roc_auc_score(test_labels, pred_probs)\n",
    "    \n",
    "    print(\"--- Evaluation Complete ---\")\n",
    "    \n",
    "    # This is the correct return dictionary that matches what our pipelines expect\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1_score\": f1,\n",
    "        \"auc\": auc,\n",
    "        \"knn_classifier\": knn,\n",
    "        \"test_embeddings\": test_embeddings,\n",
    "        \"test_labels\": test_labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf278e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pipeline A: Local PNG Files (Corrected Return Statement) ---\n",
    "\n",
    "class LocalPNGDataset(Dataset):\n",
    "    # ... (The class definition is correct and does not need to change) ...\n",
    "    def __init__(self, root_dir, filenames, image_size, split='train'):\n",
    "        self.root_dir = root_dir\n",
    "        self.filenames = filenames\n",
    "        self.split = split\n",
    "        self.image_transform = T.Compose([\n",
    "            T.Resize((image_size, image_size)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.filenames[idx]\n",
    "        image_path = os.path.join(self.root_dir, self.split, 'images', filename)\n",
    "        mask_path = os.path.join(self.root_dir, self.split, 'masks', filename)\n",
    "        if not os.path.exists(image_path): raise FileNotFoundError(f\"Image not found at: {image_path}\")\n",
    "        if not os.path.exists(mask_path): raise FileNotFoundError(f\"Mask not found at: {mask_path}\")\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        mask = Image.open(mask_path)\n",
    "        label = 1 if np.any(np.array(mask) > 0) else 0\n",
    "        return self.image_transform(image), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "def run_pipeline_local_files(train_filenames, test_filenames):\n",
    "    # ... (The setup and training loop are correct and do not need to change) ...\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"  RUNNING PIPELINE: Local PNG Files\")\n",
    "    print(\"=\"*60)\n",
    "    train_dataset = LocalPNGDataset(LOCAL_PNG_ROOT, train_filenames, IMAGE_SIZE, split='train')\n",
    "    test_dataset = LocalPNGDataset(LOCAL_PNG_ROOT, test_filenames, IMAGE_SIZE, split='train')\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    train_loader_for_eval = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    model = EmbeddingNet().to(device)\n",
    "    loss_func = losses.TripletMarginLoss(margin=0.2)\n",
    "    miner = miners.TripletMarginMiner(margin=0.2, type_of_triplets=\"semihard\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    print(\"\\n--- Starting DML Training ---\")\n",
    "    start_time = time.time()\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        pbar = tqdm(train_loader, desc=f\"Local Files - Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "        for images, labels in pbar:\n",
    "            optimizer.zero_grad()\n",
    "            embeddings = model(images.to(device))\n",
    "            hard_triplets = miner(embeddings, labels.to(device))\n",
    "            loss = loss_func(embeddings, labels.to(device), hard_triplets)\n",
    "            if loss is not None:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "    training_duration = time.time() - start_time\n",
    "    print(f\"--- Training Complete [Duration: {training_duration:.2f}s] ---\")\n",
    "    \n",
    "    # Run the evaluation\n",
    "    eval_results = evaluate_model(model, train_loader_for_eval, test_loader, device)\n",
    "    \n",
    "    # --- THE FIX IS HERE ---\n",
    "    # We now correctly construct the nested dictionary that the final report expects.\n",
    "    return {\n",
    "        \"time\": training_duration,\n",
    "        \"metrics\": {\n",
    "            \"accuracy\": eval_results[\"accuracy\"],\n",
    "            \"f1_score\": eval_results[\"f1_score\"],\n",
    "            \"auc\": eval_results[\"auc\"]\n",
    "        },\n",
    "        \"model\": model, \n",
    "        \"knn\": eval_results[\"knn_classifier\"],\n",
    "        \"embeddings\": eval_results[\"test_embeddings\"],\n",
    "        \"labels\": eval_results[\"test_labels\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf6fed0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pipeline B: Deep Lake (Corrected Return Statement) ---\n",
    "\n",
    "class DeeplakeSequentialDataset(Dataset):\n",
    "    # ... (The class definition is correct and does not need to change) ...\n",
    "    def __init__(self, ds_object, list_of_indices, image_size):\n",
    "        self.ds = ds_object\n",
    "        self.indices = list_of_indices\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize((image_size, image_size)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    def __getitem__(self, idx):\n",
    "        actual_ds_index = self.indices[idx]\n",
    "        sample = self.ds[actual_ds_index]\n",
    "        image_np = sample[\"images\"]\n",
    "        mask_np = sample[\"masks\"]\n",
    "        if not isinstance(image_np, np.ndarray): raise TypeError(f\"Expected image to be a numpy array, but got {type(image_np)}\")\n",
    "        image = Image.fromarray(image_np).convert('RGB')\n",
    "        label = 1 if np.any(mask_np > 0) else 0\n",
    "        return self.transform(image), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "def run_pipeline_deeplake(train_indices, test_indices):\n",
    "    # ... (The setup and training loop are correct and do not need to change) ...\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"  RUNNING PIPELINE: Deep Lake (Sequential Access)\")\n",
    "    print(\"=\"*60)\n",
    "    ds = deeplake.open(DEEPLAKE_PATH)\n",
    "    print(\"Finding all 'train' sample indices in the full dataset...\")\n",
    "    all_train_indices_in_ds = [i for i, s in enumerate(ds['split']) if safe_get_tensor_item_value(s) == 'train']\n",
    "    train_indices_subset = [all_train_indices_in_ds[i] for i in train_indices]\n",
    "    test_indices_subset = [all_train_indices_in_ds[i] for i in test_indices]\n",
    "    train_dataset = DeeplakeSequentialDataset(ds, train_indices_subset, IMAGE_SIZE)\n",
    "    test_dataset = DeeplakeSequentialDataset(ds, test_indices_subset, IMAGE_SIZE)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    train_loader_for_eval = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    model = EmbeddingNet().to(device)\n",
    "    loss_func = losses.TripletMarginLoss(margin=0.2)\n",
    "    miner = miners.TripletMarginMiner(margin=0.2, type_of_triplets=\"semihard\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    print(\"\\n--- Starting DML Training ---\")\n",
    "    start_time = time.time()\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        pbar = tqdm(train_loader, desc=f\"Deep Lake - Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "        for images, labels in pbar:\n",
    "            optimizer.zero_grad()\n",
    "            embeddings = model(images.to(device))\n",
    "            hard_triplets = miner(embeddings, labels.to(device))\n",
    "            loss = loss_func(embeddings, labels.to(device), hard_triplets)\n",
    "            if loss is not None:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "    training_duration = time.time() - start_time\n",
    "    print(f\"--- Training Complete [Duration: {training_duration:.2f}s] ---\")\n",
    "    \n",
    "    eval_results = evaluate_model(model, train_loader_for_eval, test_loader, device)\n",
    "\n",
    "    # --- THE FIX IS HERE ---\n",
    "    # We now correctly construct the nested dictionary that the final report expects.\n",
    "    return {\n",
    "        \"time\": training_duration,\n",
    "        \"metrics\": {\n",
    "            \"accuracy\": eval_results[\"accuracy\"],\n",
    "            \"f1_score\": eval_results[\"f1_score\"],\n",
    "            \"auc\": eval_results[\"auc\"]\n",
    "        },\n",
    "        \"model\": model, \n",
    "        \"knn\": eval_results[\"knn_classifier\"],\n",
    "        \"embeddings\": eval_results[\"test_embeddings\"],\n",
    "        \"labels\": eval_results[\"test_labels\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "123129fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Full Experiment ---\n",
      "Index setup complete. Train and test sets are now guaranteed to be separate.\n",
      "Deep Lake will use relative indices 0-7712 for training.\n",
      "Deep Lake will use relative indices 7713-9640 for testing.\n",
      "\n",
      "============================================================\n",
      "  RUNNING PIPELINE: Local PNG Files\n",
      "============================================================\n",
      "\n",
      "--- Starting DML Training ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "648ef0d7693e4bc797de13329d025cd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Local Files - Epoch 1/50:   0%|          | 0/242 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAcceleratorError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDeep Lake will use relative indices \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_indices_for_deeplake[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_indices_for_deeplake[-\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for testing.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# --- Run the Pipelines ---\u001b[39;00m\n\u001b[32m     26\u001b[39m \n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Run the pipeline for Local PNG files using the filename lists\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m local_results = \u001b[43mrun_pipeline_local_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_filenames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_filenames\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Run the pipeline for Deep Lake using the correctly partitioned index lists\u001b[39;00m\n\u001b[32m     31\u001b[39m deeplake_results = run_pipeline_deeplake(train_indices_for_deeplake, test_indices_for_deeplake)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mrun_pipeline_local_files\u001b[39m\u001b[34m(train_filenames, test_filenames)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[32m     47\u001b[39m     optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     embeddings = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m     hard_triplets = miner(embeddings, labels.to(device))\n\u001b[32m     50\u001b[39m     loss = loss_func(embeddings, labels.to(device), hard_triplets)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projetos/artigo-erbase/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projetos/artigo-erbase/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mEmbeddingNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projetos/artigo-erbase/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projetos/artigo-erbase/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projetos/artigo-erbase/venv/lib/python3.11/site-packages/torchvision/models/resnet.py:285\u001b[39m, in \u001b[36mResNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projetos/artigo-erbase/venv/lib/python3.11/site-packages/torchvision/models/resnet.py:269\u001b[39m, in \u001b[36mResNet._forward_impl\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m    267\u001b[39m     \u001b[38;5;66;03m# See note [TorchScript super()]\u001b[39;00m\n\u001b[32m    268\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.conv1(x)\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbn1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    270\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.relu(x)\n\u001b[32m    271\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.maxpool(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projetos/artigo-erbase/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projetos/artigo-erbase/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projetos/artigo-erbase/venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:173\u001b[39m, in \u001b[36m_BatchNorm.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.track_running_stats:\n\u001b[32m    171\u001b[39m     \u001b[38;5;66;03m# TODO: if statement only here to tell the jit to skip emitting this when it is None\u001b[39;00m\n\u001b[32m    172\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_batches_tracked \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_batches_tracked\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[32m    174\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.momentum \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# use cumulative moving average\u001b[39;00m\n\u001b[32m    175\u001b[39m             exponential_average_factor = \u001b[32m1.0\u001b[39m / \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_batches_tracked)\n",
      "\u001b[31mAcceleratorError\u001b[39m: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# --- The Final Execution (Corrected) ---\n",
    "\n",
    "print(\"--- Starting Full Experiment ---\")\n",
    "\n",
    "# --- Final Setup for Pair-to-Pair Comparison ---\n",
    "\n",
    "# 1. We already have 'train_filenames' and 'test_filenames' from Cell 3.\n",
    "#    These are correct and will be passed to the Local PNG pipeline.\n",
    "\n",
    "# 2. For the Deep Lake pipeline, we need a list of RELATIVE indices that correspond\n",
    "#    to the filenames. This ensures the test set is separate.\n",
    "num_train = len(train_filenames)\n",
    "num_test = len(test_filenames)\n",
    "all_relative_indices = list(range(num_train + num_test))\n",
    "\n",
    "# CORRECT: The test indices start where the train indices stop.\n",
    "train_indices_for_deeplake = all_relative_indices[:num_train]  # Will be [0, 1, ..., 999]\n",
    "test_indices_for_deeplake = all_relative_indices[num_train:]   # Will be [1000, 1001, ..., 1199]\n",
    "\n",
    "print(\"Index setup complete. Train and test sets are now guaranteed to be separate.\")\n",
    "print(f\"Deep Lake will use relative indices {train_indices_for_deeplake[0]}-{train_indices_for_deeplake[-1]} for training.\")\n",
    "print(f\"Deep Lake will use relative indices {test_indices_for_deeplake[0]}-{test_indices_for_deeplake[-1]} for testing.\")\n",
    "\n",
    "\n",
    "# --- Run the Pipelines ---\n",
    "\n",
    "# Run the pipeline for Local PNG files using the filename lists\n",
    "local_results = run_pipeline_local_files(train_filenames, test_filenames)\n",
    "\n",
    "# Run the pipeline for Deep Lake using the correctly partitioned index lists\n",
    "deeplake_results = run_pipeline_deeplake(train_indices_for_deeplake, test_indices_for_deeplake)\n",
    "\n",
    "\n",
    "# --- Final Comparison Report ---\n",
    "print(\"\\n\\n\" + \"#\"*60)\n",
    "print(\"##                  FINAL BENCHMARK RESULTS                  ##\")\n",
    "print(\"#\"*60)\n",
    "print(f\"\\nConfiguration: {len(train_filenames)} train, {len(test_filenames)} test, {NUM_EPOCHS} epochs.\")\n",
    "print(f\"Device: {device}, Num Workers: {NUM_WORKERS}\")\n",
    "\n",
    "print(\"\\n--- PERFORMANCE (TRAINING SPEED) ---\")\n",
    "print(f\"  - Local PNG Files : {local_results['time']:.2f} seconds\")\n",
    "print(f\"  - Deep Lake       : {deeplake_results['time']:.2f} seconds\")\n",
    "\n",
    "if deeplake_results['time'] > 0 and local_results['time'] > 0:\n",
    "    if local_results['time'] < deeplake_results['time']:\n",
    "        speedup = deeplake_results['time'] / local_results['time']\n",
    "        print(f\"  --> Local PNGs were {speedup:.2f}x faster.\")\n",
    "    else:\n",
    "        speedup = local_results['time'] / deeplake_results['time']\n",
    "        print(f\"  --> Deep Lake was {speedup:.2f}x faster.\")\n",
    "\n",
    "print(\"\\n--- EFFECTIVENESS (k-NN on Embeddings) ---\")\n",
    "print(\"                     | Local Files | Deep Lake\")\n",
    "print(\"---------------------|-------------|-----------\")\n",
    "print(f\" Accuracy            | {local_results['metrics']['accuracy']:.4f}      | {deeplake_results['metrics']['accuracy']:.4f}\")\n",
    "print(f\" F1-Score            | {local_results['metrics']['f1_score']:.4f}      | {deeplake_results['metrics']['f1_score']:.4f}\")\n",
    "print(f\" AUC Score           | {local_results['metrics']['auc']:.4f}      | {deeplake_results['metrics']['auc']:.4f}\")\n",
    "print(\"\\n\" + \"#\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f861639e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction function `predict_single_image` is now defined.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 8: Define the Single Image Prediction Function ---\n",
    "\n",
    "def predict_single_image(model, knn, image_path, device, image_size):\n",
    "    \"\"\"Loads a single image, gets its embedding, and classifies it with the k-NN.\"\"\"\n",
    "    \n",
    "    # Define the same transformations used during training\n",
    "    transform = T.Compose([\n",
    "        T.Resize((image_size, image_size)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Load and transform the image\n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Image not found at {image_path}\")\n",
    "        return\n",
    "\n",
    "    # The model expects a batch, so we add a \"batch\" dimension of 1\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Get the embedding\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        embedding = model(image_tensor).cpu().numpy()\n",
    "        \n",
    "    # Get the prediction from the k-NN\n",
    "    prediction = knn.predict(embedding)[0]\n",
    "    prediction_prob = knn.predict_proba(embedding)[0]\n",
    "    \n",
    "    # Map prediction to human-readable format\n",
    "    class_map = {0: \"No Egg\", 1: \"Egg\"}\n",
    "    predicted_class = class_map[prediction]\n",
    "    \n",
    "    print(f\"\\n--- Prediction for: {os.path.basename(image_path)} ---\")\n",
    "    print(f\"Predicted Class: {predicted_class}\")\n",
    "    print(f\"Confidence (Probability of being 'Egg'): {prediction_prob[1]:.2%}\")\n",
    "\n",
    "print(\"Prediction function `predict_single_image` is now defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b05cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################################################\n",
      "##                 INFERENCE COMPARISON                  ##\n",
      "############################################################\n",
      "\n",
      "--- Using Model Trained on Local PNGs ---\n",
      "\n",
      "--- Prediction for: 6259.png ---\n",
      "Predicted Class: No Egg\n",
      "Confidence (Probability of being 'Egg'): 0.00%\n",
      "\n",
      "--- Using Model Trained on Deep Lake ---\n",
      "\n",
      "--- Prediction for: 6259.png ---\n",
      "Predicted Class: No Egg\n",
      "Confidence (Probability of being 'Egg'): 0.00%\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 9: Side-by-Side Inference Comparison ---\n",
    "\n",
    "# Use the prediction function defined in Cell 8\n",
    "# predict_single_image(model, knn, image_path, device, image_size)\n",
    "\n",
    "# Pick a single image to test\n",
    "test_image_path = os.path.join(LOCAL_PNG_ROOT, 'train', 'images', test_filenames[50]) # Pick the 51st test image\n",
    "\n",
    "print(\"\\n\" + \"#\"*60)\n",
    "print(\"##                 INFERENCE COMPARISON                  ##\")\n",
    "print(\"#\"*60)\n",
    "\n",
    "# --- Prediction using the PNG-trained model ---\n",
    "print(\"\\n--- Using Model Trained on Local PNGs ---\")\n",
    "predict_single_image(\n",
    "    local_results['model'], \n",
    "    local_results['knn'], \n",
    "    test_image_path, \n",
    "    device, \n",
    "    IMAGE_SIZE\n",
    ")\n",
    "\n",
    "# --- Prediction using the Deep Lake-trained model ---\n",
    "print(\"\\n--- Using Model Trained on Deep Lake ---\")\n",
    "predict_single_image(\n",
    "    deeplake_results['model'], \n",
    "    deeplake_results['knn'], \n",
    "    test_image_path, \n",
    "    device, \n",
    "    IMAGE_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a62d9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'local_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     44\u001b[39m fig.suptitle(\u001b[33m'\u001b[39m\u001b[33mEmbedding Space Visualization Comparison\u001b[39m\u001b[33m'\u001b[39m, fontsize=\u001b[32m16\u001b[39m)\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# --- Plot for the PNG-trained model ---\u001b[39;00m\n\u001b[32m     47\u001b[39m create_tsne_plot(\n\u001b[32m     48\u001b[39m     ax1, \n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[43mlocal_results\u001b[49m[\u001b[33m'\u001b[39m\u001b[33membeddings\u001b[39m\u001b[33m'\u001b[39m], \n\u001b[32m     50\u001b[39m     local_results[\u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m], \n\u001b[32m     51\u001b[39m     title=\u001b[33m\"\u001b[39m\u001b[33mTrained on Local PNGs\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     52\u001b[39m )\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# --- Plot for the Deep Lake-trained model ---\u001b[39;00m\n\u001b[32m     55\u001b[39m create_tsne_plot(\n\u001b[32m     56\u001b[39m     ax2, \n\u001b[32m     57\u001b[39m     deeplake_results[\u001b[33m'\u001b[39m\u001b[33membeddings\u001b[39m\u001b[33m'\u001b[39m], \n\u001b[32m     58\u001b[39m     deeplake_results[\u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m], \n\u001b[32m     59\u001b[39m     title=\u001b[33m\"\u001b[39m\u001b[33mTrained on Deep Lake\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     60\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'local_results' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x900 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Cell 10: Side-by-Side Visualization Comparison (Corrected) ---\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def create_tsne_plot(ax, embeddings, labels, title):\n",
    "    \"\"\"Helper function to create one t-SNE plot on a given matplotlib axis.\"\"\"\n",
    "    print(f\"--- Running t-SNE for '{title}' ---\")\n",
    "    \n",
    "    # --- THE FIX IS HERE ---\n",
    "    # In modern scikit-learn, 'n_iter' is replaced by 'max_iter'.\n",
    "    # We also specify 'init' and 'learning_rate' which are good defaults.\n",
    "    tsne = TSNE(\n",
    "        n_components=2, \n",
    "        perplexity=30, \n",
    "        init='pca', \n",
    "        max_iter=1000, # Use max_iter instead of n_iter\n",
    "        learning_rate='auto',\n",
    "        n_jobs=-1 # Use all available CPU cores to speed up t-SNE\n",
    "    )\n",
    "    # -----------------------\n",
    "    \n",
    "    embeddings_2d = tsne.fit_transform(embeddings)\n",
    "    \n",
    "    class_map = {0: \"No Egg\", 1: \"Egg\"}\n",
    "    colors = {0: 'blue', 1: 'red'}\n",
    "    \n",
    "    for class_id, class_name in class_map.items():\n",
    "        indices = np.where(labels == class_id)\n",
    "        ax.scatter(\n",
    "            embeddings_2d[indices, 0], \n",
    "            embeddings_2d[indices, 1], \n",
    "            c=colors[class_id],\n",
    "            label=class_name,\n",
    "            alpha=0.7\n",
    "        )\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"t-SNE Component 1\")\n",
    "    ax.set_ylabel(\"t-SNE Component 2\")\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "# Create a figure with two subplots, side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 9))\n",
    "fig.suptitle('Embedding Space Visualization Comparison', fontsize=16)\n",
    "\n",
    "# --- Plot for the PNG-trained model ---\n",
    "create_tsne_plot(\n",
    "    ax1, \n",
    "    local_results['embeddings'], \n",
    "    local_results['labels'], \n",
    "    title=\"Trained on Local PNGs\"\n",
    ")\n",
    "\n",
    "# --- Plot for the Deep Lake-trained model ---\n",
    "create_tsne_plot(\n",
    "    ax2, \n",
    "    deeplake_results['embeddings'], \n",
    "    deeplake_results['labels'], \n",
    "    title=\"Trained on Deep Lake\"\n",
    ")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3803fc54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################################################\n",
      "##                     SAVING FINAL RESULTS                     ##\n",
      "############################################################\n",
      "Results will be saved in the 'experiment_results' directory.\n",
      "\n",
      "Final metrics successfully saved to: experiment_results/final_comparison_metrics.csv\n",
      "\n",
      "--- Content of CSV File ---\n",
      "  Pipeline  Training Time (s)  Accuracy  F1-Score  AUC Score\n",
      "Local PNGs       17469.215718  0.847510  0.563798   0.772518\n",
      " Deep Lake       21140.231081  0.833506  0.545969   0.771607\n",
      "\n",
      "Model trained on PNGs saved to: experiment_results/model_trained_on_pngs.pth\n",
      "Model trained on Deep Lake saved to: experiment_results/model_trained_on_deeplake.pth\n",
      "\n",
      "Embeddings from PNG model saved to: experiment_results/embeddings_from_png_model.npz\n",
      "Embeddings from Deep Lake model saved to: experiment_results/embeddings_from_deeplake_model.npz\n",
      "\n",
      "############################################################\n",
      "##                      SAVE COMPLETE                      ##\n",
      "############################################################\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"\\n\" + \"#\"*60)\n",
    "print(\"##                     SAVING FINAL RESULTS                     ##\")\n",
    "print(\"#\"*60)\n",
    "\n",
    "# --- 1. Create a Directory for the Results ---\n",
    "output_dir = \"experiment_results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Results will be saved in the '{output_dir}' directory.\")\n",
    "\n",
    "\n",
    "# --- 2. Save the Final Metrics to a CSV File ---\n",
    "\n",
    "# First, let's create a flat dictionary for easier DataFrame creation\n",
    "summary_data = {\n",
    "    'Pipeline': ['Local PNGs', 'Deep Lake'],\n",
    "    'Training Time (s)': [local_results['time'], deeplake_results['time']],\n",
    "    'Accuracy': [local_results['metrics']['accuracy'], deeplake_results['metrics']['accuracy']],\n",
    "    'F1-Score': [local_results['metrics']['f1_score'], deeplake_results['metrics']['f1_score']],\n",
    "    'AUC Score': [local_results['metrics']['auc'], deeplake_results['metrics']['auc']]\n",
    "}\n",
    "\n",
    "# Convert to a pandas DataFrame for easy saving\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Define the path and save the CSV\n",
    "csv_path = os.path.join(output_dir, \"final_comparison_metrics.csv\")\n",
    "summary_df.to_csv(csv_path, index=False, float_format='%.4f')\n",
    "\n",
    "print(f\"\\nFinal metrics successfully saved to: {csv_path}\")\n",
    "print(\"\\n--- Content of CSV File ---\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "\n",
    "# --- 3. Save the Trained Models (State Dictionaries) ---\n",
    "# Saving the model's 'state_dict' is the recommended PyTorch way.\n",
    "# It's just the learned weights, not the entire model structure.\n",
    "\n",
    "# Save the model trained on PNGs\n",
    "local_model_path = os.path.join(output_dir, \"model_trained_on_pngs.pth\")\n",
    "torch.save(local_results['model'].state_dict(), local_model_path)\n",
    "print(f\"\\nModel trained on PNGs saved to: {local_model_path}\")\n",
    "\n",
    "# Save the model trained on Deep Lake\n",
    "deeplake_model_path = os.path.join(output_dir, \"model_trained_on_deeplake.pth\")\n",
    "torch.save(deeplake_results['model'].state_dict(), deeplake_model_path)\n",
    "print(f\"Model trained on Deep Lake saved to: {deeplake_model_path}\")\n",
    "\n",
    "\n",
    "# --- 4. (Optional) Save Embeddings for Later Visualization ---\n",
    "# This is useful if you want to re-create the t-SNE plots without retraining.\n",
    "local_embeddings_path = os.path.join(output_dir, \"embeddings_from_png_model.npz\")\n",
    "np.savez(local_embeddings_path, embeddings=local_results['embeddings'], labels=local_results['labels'])\n",
    "print(f\"\\nEmbeddings from PNG model saved to: {local_embeddings_path}\")\n",
    "\n",
    "deeplake_embeddings_path = os.path.join(output_dir, \"embeddings_from_deeplake_model.npz\")\n",
    "np.savez(deeplake_embeddings_path, embeddings=deeplake_results['embeddings'], labels=deeplake_results['labels'])\n",
    "print(f\"Embeddings from Deep Lake model saved to: {deeplake_embeddings_path}\")\n",
    "\n",
    "print(\"\\n\" + \"#\"*60)\n",
    "print(\"##                      SAVE COMPLETE                      ##\")\n",
    "print(\"#\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7293b75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
